% !TEX root =  ../main.tex 

\section{A framework for personalized biopsy schedules}
\label{sec : framework_pers_biop_sched}
The first step in creating a personalized schedule for biopsies is to come up with a model for Gleason scores, PSA levels and other subject specific characteristics. In PRIAS, PSA levels are measured at the time of induction, every 3 months for the first 2 years in the study and then every 6 months thereafter. Thus PSA levels can be modeled as a longitudinal outcome. As mentioned earlier, patients in PRIAS have a Gleason score of 6 or less at the time of induction in the study, and patients are removed from AS the first time Gleason reclassification takes place. Since our interest also lies in time to Gleason reclassification, we model it as a time to event outcome. i.e. time to Gleason reclassification. While univariate modeling of the two aforementioned outcomes can be done separately using longitudinal model and survival models, it is important to note that they are not independent. To model the association between the two types of outcomes we use a joint model for time to event and longitudinal outcomes.

\subsection{Joint model for Gleason reclassification and PSA levels}
\label{subsec : jm_definition}
Let $T_i^*$ denote the true Gleason reclassification time for the $i^{th}$ subject. Let the times at which biopsies are conducted for the $i^{th}$ patient be denoted by $C_i = \{C_{i0}, C_{i1}, ... C_{ig_i}; C_{ij} < C_{ik}, \forall j<k \}$. $T_i^*$ cannot be observed directly and it is only known that it falls in an interval $(l_i, r_i]$, where $l_i = C_{i(g_i-1)}, r_i = C_{ig_i}$ if Gleason reclassification is observed and $l_i = C_{ig_i}, r_i=\infty$ if patient drops out. The latter is also known as right censoring. Let $\boldsymbol{y}_i$ denote the $n_i \times 1$ longitudinal outcome vector for the PSA levels of the $i^{th}$ subject. The population of interest is all the patients enrolled in AS. For a sample of $n$ patients from this population the complete data is denoted by $\mathcal{D}_n = \{T_i, l_i, r_i, \boldsymbol{y}_i; i = 1,...n\}$, where $T_i \epsilon (l_i, r_i]$ denotes the observed time of Gleason reclassification.\\

To model the evolution of the longitudinal outcome, which is PSA levels in the case at hand, the joint model utilizes a linear mixed effects model. The longitudinal outcome $y_i(t)$ at time $t$ is modeled as:

\begin{equation*}
\begin{split}
y_i(t) &= m_i(t) + \varepsilon_i(t), \\
&= \boldsymbol{x}_i^T(t) \beta + \boldsymbol{z}_i^T(t) \boldsymbol{b}_i + \varepsilon_i(t)
\end{split}
\end{equation*}
%Do not introduce a space here, otherwise it starts a new paragraph
where, $m_i(t)$ denotes the true and unobserved value of the longitudinal outcome at time $t$. $\varepsilon_i(t) \sim N(0, \sigma^2)$ denotes the measurement error term, assumed normally distributed with variance $\sigma^2$. $\beta$ denotes the vector of the unknown fixed-effects parameters. $\boldsymbol{b}_i \sim N(0, \boldsymbol{D})$ denotes the vector of random effects, assumed normally distributed with mean zero and covariance matrix D, and independent of $\varepsilon_i(t)$. $\boldsymbol{x}_i(t)$ and $\boldsymbol{z}_i(t)$ denote row vectors of the design matrices for the fixed and random effects, respectively. For non continuous longitudinal outcomes joint models utilize Generalized linear mixed models \citep{rizopoulos2012joint}.\\

Since both PSA levels and Gleason scores are affected by the state of prostate cancer, they are inherently correlated with each other. To this end, the joint model utilizes a relative risk sub-model where the hazard of Gleason reclassification $h_i(t)$ at any time point $t$ depends on the history of true and unobserved values of PSA levels $\mathcal{M}_i(t) = \{m_i(v), 0\leq v \leq t\}$ measured up to that time point. Joint models offer flexibility in modeling this dependence. In its simplest form, the hazard may depend on instantaneous value of PSA $m_i(t)$ at time $t$. More sophisticated ones are dependence of hazard at time $t$ on PSA-DT, PSA velocity $m'_i(t) = \dfrac{d m_i(t)}{dt}$, or even on the cumulative effect of PSA $\int_0^t m_i(s) \,ds$ up to $t$. The fact that any functional form of dependence is possible, is evident from the following equation:

\begin{equation*}
h_i(t \mid M_i(t), \boldsymbol{w}_i) = h_0(t) e^{\gamma^T\boldsymbol{w}_i + f\{M_i(t), \boldsymbol{b}_i, \alpha\}}
\end{equation*}
where $h_0(t)$ is the baseline hazard at time $t$. $\boldsymbol{w}_i$ is a vector of baseline covariates and $\gamma$ are the corresponding parameters. The function $f(\cdot)$ parametrized by vector $\alpha$ specifies the features of longitudinal outcome that are are included in the linear predictor of the relative risk model.\\

While $\alpha$ controls the strength of association between the hazard of reclassification and features of the PSA levels, the fact that both Gleason scores and PSA levels are internally related to a patient's health, is manifested by the random effects $\boldsymbol{b}_i$ in the model. The joint model postulates that given the random effects, time to Gleason reclassification and PSA levels measured at different time points are mutually independent. As mentioned earlier, in PRIAS study PSA-DT is used to decide the schedule of biopsies. Although PSA-DT is computed using observed PSA values, dependence on observed longitudinal history $\mathcal{Y}(t) = \{y_i(v), 0\leq v \leq t\}$ at any time $t$, is not the same as dependence on patient's health. This because dependence on patient's health, manifested by $\boldsymbol{b}_i$ is same as dependence on future unobserved values of PSA. Thus the inference for the parameters of interest $\theta$ doesn't change even if uncertainty in biopsy schedule $C_i$ is not modeled. The kernel of the corresponding joint likelihood conditional on the random effects and the model parameters is given by:

\begin{equation*}
p(T_i, l_i, r_i, \boldsymbol{y}_i \mid \boldsymbol{b}_i, \theta) \propto p(T_i \mid l_i, r_i, \boldsymbol{b}_i, \theta) p(\boldsymbol{y}_i \mid \boldsymbol{b}_i, \theta)
\end{equation*}

\subsection{Personalized scheduling approaches}
\label{subsec : pers_sched_approaches}
Once a joint model for Gleason reclassification and PSA levels is obtained, the next step is to use it to create personalized schedules for biopsies. In this section we present the various personalized biopsy scheduling approaches and their motivation. The personalized schedules that we propose are dynamic in nature and thus at any given time, only 1 future biopsy is scheduled. The age of the patient and entire PSA, repeat biopsy history up to that time point is considered while computing the time of next biopsy. To elucidate the scheduling methods, let us assume that the a personalized schedule is to be created a new patient numbered $j$ who is not present in the original sample of patients $\mathcal{D}_n$. Further let us assume that the patient did not have a Gleason reclassification at their last biopsy, performed at time $t$ and that the PSA measurements are available for the patient up to a time point $s > t$. Combining these two pieces of information, the predictive distribution $g(T^*_j)$ for time to Gleason reclassification for this patient is given by (conditioning on baseline covariates $\boldsymbol{w}_i$ is dropped for notational simplicity here onwards):

\begin{equation}
\label{eq : dyn_dist_fail_time}
\begin{split}
g(T^*_j) &= p(T^*_j \mid T^*_j > t, \mathcal{Y}_j(s), \mathcal{D}_n)\\
%&= \int p(T^*_j \mid T^*_j > t, \mathcal{Y}_j(s), \theta) p(\theta \mid \mathcal{D}_n) \,d\theta\\
%&= \int \int p(T^*_j \mid T^*_j > t, \mathcal{Y}_j(s), \boldsymbol{b_j}, \theta) p(\boldsymbol{b}_j \mid T^*_j>t, \mathcal{Y}_j(s), \theta)p(\theta \mid \mathcal{D}_n) \,d\boldsymbol{b}_j \,d\theta
\end{split}
\end{equation}
where $\mathcal{Y}_j(s)$ denotes the history of PSA measurements done up to time $s$. Given the predictive distribution, our goal is find the optimal time $u \geq s$ of the next biopsy. To achieve this we use principles from statistical decision theory in a Bayesian setting \citep{bergerDecisionTheory,robertBayesianChoice}. More specifically, we propose to choose future biopsy time $u$ by minimizing the posterior expected loss $E_g[L(T^*_j, u)]$, where the expectation is taken w.r.t. the predictive distribution $g(T^*_j)$. 

\begin{equation*}
E_g[L(T^*_j, u)] = \int_t^\infty L(T^*_j, u) p(T^*_j \mid T^*_j > t, \mathcal{Y}_j(s), \mathcal{D}_n) \,dT^*_j
\end{equation*}
Various loss functions $L(T^*_j, u)$ have been proposed in literature \citep{robertBayesianChoice}. We next present the loss functions we use in this work and the motivation for their choice.

\subsubsection{Expected time of Gleason reclassification}
\label{subsubsec : exp_fail_time}
One of the reasons, patients did not comply with the existing PRIAS schedule was \textquoteleft complications on a previous biopsy \textquoteright. Therefore, it makes sense to have as less biopsies as possible. In the ideal case only 1 biopsy, performed at the exact time of Gleason reclassification is sufficient. In this regard, the squared loss function $L(T^*_j, u) = (T^*_j - u)^2$ has the property that the loss increases quadratically as the error $T^*_j - u$ increases. So the squared loss function satisfies our requirement of choosing a $u$ as close to the true Gleason reclassification time as possible. The posterior expected loss is given by:

\begin{equation}
\label{eq : posterior_squared_loss}
\begin{split}
E_g[L(T^*_j, u)] &= E_g[(T^*_j - u)^2]\\
&=E_g[(T^*_j)^2] + u^2 -2uE_g[T^*_j]
\end{split}
\end{equation}
The posterior expected loss in equation \ref{eq : posterior_squared_loss} attains its minimum at $u = E_g[T^*_j]$, also known as expected time of Gleason reclassification.

\subsubsubsection{Estimation}
Since there is no closed form solution available for $E_g[T^*_j]$, for its estimation we introduce a construct called dynamic survival probability \citep{rizopoulos2011dynamic}. The dynamic survival probability $\pi_j(v \mid t, s)$ of patient $j$ is the survival probability at time $v$, conditional on the observed PSA history $\mathcal{Y}_j(s)$ and the fact that the patient did not have Gleason reclassification up to $t$. It is given by:

\begin{equation}
\pi_j(v \mid t, s) = Pr(T^*_j \geq v \mid  T^*_j >t, \mathcal{Y}_j(s), D_n), v \geq t
\end{equation}
The relationship between expected time of Gleason reclassification and dynamic survival probability is given by:

\begin{equation*}
E_g[T^*_j] = t + \int_t^\infty \pi_j(v \mid t, s) \,dv
\end{equation*}
Since the R package JMbayes already provides an implementation of $\pi_j(v \mid t, s)$, this approach was preferred over Monte Carlo methods to estimate $E_g[T^*_j]$ from the predictive distribution $g(T^*_j)$. A limitation of expected time of Gleason reclassification though, is that it is only useful when the variance of predictive distribution $g(T^*_j)$ is small. The variance is given by:

\begin{equation}
\begin{split}
Var_g[T^*_j] &= E_g[{T^*_j}^2] - {E_g[T^*_j]}^2\\
&= 2 \int_t^\infty {(v-t) \pi_j(v \mid t, s) \,dv} - {\bigg(\int_t^\infty \pi_j(v \mid t, s) \,dv\bigg)}^2
\end{split}
\end{equation}
It is to be noted that the posterior expected loss with the squared loss function is equal to the variance. Since the variance is same as expected loss....loss increases....intuitive??
\textcolor{red}{\textbf{THEORETICALLY PROVE HOW VARIANCE DECREASES WITH LESS Y(s)}}

\subsubsection{Dynamic risk of reclassification}
\label{subsubsec : dynamic_risk_definitions}
In a practical scenario it is possible that a doctor or a patient may not want to exceed a certain risk of reclassification $1 - \pi_j(u \mid t, s)$ since the last biopsy. The personalized scheduling approach based on dynamic risk of reclassification, schedules the next biopsy at a time point $u$ such that the dynamic risk of reclassification is higher than a certain threshold $1-\kappa,\ \kappa \epsilon [0,1]$ beyond $u$. Or in other words the dynamic survival probability $\pi_j(u \mid t, s)$ is below a threshold $\kappa$ beyond $u$. In this regard, the posterior expected loss for the following multilinear loss function can be minimized to find the most optimal $u$:

\begin{equation}
\label{eq : loss_dynamic_risk}
L_{k_1, k_2}(T^*_j, u) =
    \begin{cases}
      k_2(T^*_j-u) & if(T^*_j > u)\\
      k_1(u-T^*_j) & \text{otherwise}
    \end{cases}       
\end{equation}
where $k_1 > 0$, $k_2 > 0$ are constants parameterizing the loss function. The posterior expected loss function $E_g[L_{k_1, k_2}(T^*_j, u)]$ obtains its minimum at $u = \pi_j^{-1}\Big\{\dfrac{k_1}{k_1 + k_2}\Big\}$ \citep{robertBayesianChoice}. The choice of $k_1, k_2$ is equivalent to the choice of $\kappa$. More specifically, $\kappa = \dfrac{k_1}{k_1 + k_2}$. 

\subsubsubsection{Choice of $\kappa$}
If $\kappa$ is required to be chosen on the basis of doctor's advice; E.g. $1 - \kappa = 0.5$, then any of $k_1, k_2 \mid k_1=k_2$ are acceptable.\\

While expert advice can be invaluable, it is also possible to automate the choice of $\kappa$. We propose to choose a $\kappa$ for which a binary classification accuracy measure \citep{lopez2014optimalcutpoints,sokolova2009systematic}, discriminating between cases and controls, is maximized. In PRIAS, cases are patients who experience Gleason reclassification and the rest are controls. However, a patient can be in control group at some time $t_a$ and in the cases at some future time point $t_b > t_a$, and thus time dependent binary classification is more relevant. In joint models, a subject $j$ is predicted to be a case if $\pi_j(t + \Delta t \mid t,s) \leq \kappa$ and a control if $\pi_j(t + \Delta t \mid t,s) > \kappa$ \citep{rizopoulosJMbayes}. The time window $\Delta t$ can be either chosen on a clinical basis (such as 1 year in PRIAS \textcolor{red}{\textbf{WHY 1 YEAR}}) or it can be chosen at a point where $AUC(t, \Delta t, s)$ \citep{rizopoulosJMbayes} is largest. i.e. $\Delta t$ for which the model has the most discriminative capability at time $t$. The binary classification accuracy measures we maximize to select the threshold $\kappa$ are the following (the binary classification measures depend on $t, \Delta t, s$, although the notation is droppped for readability):

\begin{itemize}
\item Accuracy: $ACC = \dfrac{TP + TN}{TP + FP + TN + FN}$, where TP, FP, TN and FN are the number of true positives, false positives, true negatives and false negatives at time point $t$. In this case if $k_1 = TP + TN$ and $k_2 = FP + FN$, then $\argmax{k_1, k_2} ACC$ gives the optimal $k_1, k_2$ or equivalently the $\kappa$.

\item Youden's index: $J = \text{Sensitivity} + \text{Specificity}- 1$,\\
where sensitivity is defined as $Pr(\pi_j(t + \Delta t \mid t,s) \leq \kappa \mid T^*_j \epsilon (t, t + \Delta t])$ and specificity is defined as $Pr(\pi_j(t + \Delta t \mid t,s) > \kappa \mid T^*_j > t + \Delta t)$. In this case if $k_1 = FP \cdot TP - FN \cdot TN$ and $k_2 = (TP+FN)(FP+TN) - k_1$, then $\argmax{k_1, k_2} J$ gives the optimal $k_1, k_2$ or equivalently the $\kappa$.

\item F1 Score: $F1 = \dfrac{2TP}{2TP + FP + FN}$. In this case if $k_1 = 2TP$ and $k_2 = FP + FN$, then $\argmax{k_1, k_2} F1$ gives the optimal $k_1, k_2$ or equivalently the $\kappa$.
\end{itemize}

\subsection{the flow chart and the definition of offset based on the flow chart}